---
phase: 02-pay-and-run
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - src/lib/benchmark/engine.ts
  - src/lib/email/send-report-ready.ts
  - src/emails/report-ready.tsx
  - package.json
autonomous: true

must_haves:
  truths:
    - "Benchmark engine iterates over selected models and runs each against all images with concurrency control"
    - "Engine enforces per-model concurrency limit of 3 and global concurrency limit of 10"
    - "Engine aborts remaining work when cost ceiling is reached and marks report as partial"
    - "Engine gracefully shuts down when approaching 750s elapsed time"
    - "Each completed run is written to benchmark_runs table in real-time"
    - "Report status transitions: paid -> running -> complete (or failed)"
    - "Report completion email is sent via Resend with link to report; email failure is non-fatal and does not affect benchmark results"
    - "Mock email mode logs email content instead of sending"
    - "Engine calculates recommended model from results using priority-weighted scoring"
  artifacts:
    - path: "src/lib/benchmark/engine.ts"
      provides: "Main benchmark orchestration loop with concurrency control, cost tracking, and graceful shutdown"
      exports: ["runBenchmark"]
    - path: "src/lib/email/send-report-ready.ts"
      provides: "Send report completion email via Resend"
      exports: ["sendReportReadyEmail"]
    - path: "src/emails/report-ready.tsx"
      provides: "React Email template for report completion notification"
      exports: ["default"]
  key_links:
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/benchmark/runner.ts"
      via: "Calls runModelBenchmark for each model+image combination"
      pattern: "runModelBenchmark"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/benchmark/cost-tracker.ts"
      via: "Creates CostTracker instance and passes to runner"
      pattern: "new CostTracker"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/benchmark/json-compare.ts"
      via: "Uses compareStrict/compareRelaxed and calculateFieldAccuracy for scoring"
      pattern: "compareStrict|calculateFieldAccuracy"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/supabase/admin.ts"
      via: "Uses admin client for database operations in background task"
      pattern: "createAdminClient"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/email/send-report-ready.ts"
      via: "Sends completion email after benchmark finishes"
      pattern: "sendReportReadyEmail"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/wizard/cost-estimator.ts"
      via: "Final budget enforcement before execution (defense-in-depth)"
      pattern: "estimateCost|optimizeRunsForBudget"
    - from: "src/app/api/webhooks/stripe/route.ts"
      to: "src/lib/benchmark/engine.ts"
      via: "Webhook triggers runBenchmark in after() callback"
      pattern: "runBenchmark"
---

<objective>
Build the benchmark engine orchestration loop and report completion email. The engine coordinates running all selected models against all images with concurrency limits, cost ceiling enforcement, and graceful time-based shutdown, then calculates final results and sends a completion email.

Purpose: This is the core execution engine that delivers the benchmark report value. It ties together the runner, cost tracker, JSON comparison, and database to produce comprehensive per-model results.

Output: Working benchmark engine that can be triggered from the webhook handler, executes all benchmarks, records results, and sends a completion email.
</objective>

<execution_context>
@/Users/lukelibraro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lukelibraro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-pay-and-run/02-CONTEXT.md
@.planning/phases/02-pay-and-run/02-RESEARCH.md
@.planning/phases/02-pay-and-run/02-01-SUMMARY.md
@.planning/phases/02-pay-and-run/02-02-SUMMARY.md

Key files from prior plans:
@src/lib/benchmark/runner.ts
@src/lib/benchmark/json-compare.ts
@src/lib/benchmark/cost-tracker.ts
@src/lib/benchmark/backoff.ts
@src/lib/supabase/admin.ts
@src/lib/config/models.ts
@src/lib/config/constants.ts
@src/types/database.ts
@src/types/benchmark.ts
@src/app/api/webhooks/stripe/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Benchmark engine orchestration loop with concurrency control, cost ceiling, and graceful shutdown</name>
  <files>
    src/lib/benchmark/engine.ts
    package.json
  </files>
  <action>
    Install p-limit: `npm install p-limit`

    **Create `src/lib/benchmark/engine.ts`:**

    Main export: `async function runBenchmark(reportId: string): Promise<void>`

    **Step 1: Load report and prepare execution plan**
    - Use createAdminClient() (service role, no user session in background task)
    - Load report by ID, verify status is 'paid'
    - Update report status to 'running', set started_at to now()
    - Extract from config_snapshot: selected_models (model IDs), extraction_prompt, json_schema, image_paths
    - Load expected JSON for each image from the draft's upload_data (via draft_id reference)
    - Build execution plan: array of { modelId, imageIndex, imageUrl, expectedJson } for all combinations

    **Step 2: Pre-calculate budget and enforce limits**
    - For each model ID, look up ModelInfo from CURATED_MODELS via getModelById()
    - Read runsPerModel from config_snapshot (may already be reduced by checkout API's budget optimization)
    - Import `estimateCost`, `optimizeRunsForBudget` from `@/lib/wizard/cost-estimator`
    - Calculate estimated total cost using estimateCost()
    - Log the projected cost vs budget ceiling
    - **Final budget enforcement** (defense-in-depth, checkout API does primary validation):
      - If projected cost > API_BUDGET_CEILING ($7): recalculate optimized runs via optimizeRunsForBudget(), use the reduced count for execution, log the adjustment
      - If even 1 run per model exceeds budget: mark report as 'failed' with error "Configuration exceeds budget at minimum runs", return early
    - Create CostTracker with soft ceiling (API_BUDGET_CEILING = $7) and hard ceiling (HARD_COST_CEILING = $15)

    **Step 3: Set up concurrency control**
    - Import pLimit from 'p-limit'
    - Create per-model limiters: Map<modelId, ReturnType<typeof pLimit>> with PER_MODEL_CONCURRENCY (3)
    - Create global limiter with GLOBAL_CONCURRENCY (10)
    - Track start time for graceful shutdown at 750s

    **Step 4: Execute benchmark runs**
    - Create all run tasks as promises. For each model + image combination:
      ```typescript
      const task = globalLimit(() => modelLimit(() => executeRun(params)));
      ```
    - `executeRun` function:
      1. Check if costTracker.shouldAbort() -- if true, skip (record as 'skipped')
      2. Check elapsed time -- if > 750s, skip (graceful shutdown)
      3. Insert benchmark_run record with status 'running'
      4. Call runModelBenchmark() from runner.ts
      5. Compare output against expected JSON:
         - exactMatch via compareStrict(expectedJson, outputJson)
         - Calculate field accuracy via calculateFieldAccuracy(expectedJson, outputJson, 'strict')
         - Calculate field errors via diffFields(expectedJson, outputJson, 'strict')
      6. Update benchmark_run record with results:
         - output_json, is_valid_json, exact_match, field_accuracy, field_errors
         - response_time_ms, input_tokens, output_tokens, actual_cost, estimated_cost
         - status: 'complete' (or 'failed' if runner threw)
      7. On error: Update benchmark_run with status 'failed', error_message
    - Use Promise.allSettled() to wait for all tasks (don't fail on individual errors)

    **Step 5: Calculate aggregate results**
    - Query all benchmark_runs for this report
    - Group by model_id
    - For each model, calculate:
      - accuracy: average field_accuracy across all runs
      - exactMatchRate: count(exact_match=true) / total runs
      - costPerCall: average actual_cost
      - medianLatency: median of response_time_ms
      - p95Latency: 95th percentile of response_time_ms
      - spread: standard deviation of field_accuracy
      - runsCompleted: count(status='complete')
      - runsAttempted: total count
      - fieldErrors: aggregate and count occurrences of each unique field error

    **Step 6: Determine recommended model**
    - Reuse scoring logic from model-recommender.ts concepts:
      - Load priority ranking from config_snapshot
      - Score each model by: accuracy (weight by priority position), cost efficiency, speed
      - Top-scoring model = recommended
    - Update report: recommended_model, total_api_cost (from costTracker.getSpent()), model_count
    - Log predicted vs actual cost for tracking

    **Step 7: Complete report**
    - Update report status to 'complete', set completed_at
    - If any runs were skipped due to cost ceiling: add a note field or keep model_count reflecting actual
    - Call sendReportReadyEmail() (from Task 2)

    **Error handling:**
    - Wrap entire function in try/catch
    - On unrecoverable error: update report status to 'failed' with error details
    - Always ensure report gets a terminal status (complete or failed), never left as 'running'

    **Logging:**
    - Use console.log with structured prefix: `[benchmark:${reportId}]`
    - Log: start, each model completion, cost milestones, abort events, completion
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `npm run build` succeeds
    - engine.ts exports runBenchmark function
    - Function loads report, executes runs with concurrency, records results, completes report
    - Mock mode (DEBUG_MOCK_OPENROUTER=true) runs to completion without real API calls
  </verify>
  <done>
    Benchmark engine orchestrates all model runs with per-model (3) and global (10) concurrency limits via p-limit. Cost tracker prevents overspend with $7 soft ceiling and $15 hard abort. Graceful shutdown at 750s elapsed. Each run is recorded to database. Aggregate results calculated and recommended model determined. Report transitions from paid -> running -> complete/failed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Report completion email via Resend with React Email template</name>
  <files>
    src/lib/email/send-report-ready.ts
    src/emails/report-ready.tsx
    package.json
  </files>
  <action>
    Install email packages: `npm install resend @react-email/components`

    **1. React Email template (`src/emails/report-ready.tsx`):**
    Create a React Email component for the report completion notification.
    Uses @react-email/components: Html, Head, Body, Container, Section, Text, Button, Hr, Img, Preview.

    Template content:
    - Preview text: "Your ModelPick benchmark report is ready"
    - ModelPick logo/wordmark text header
    - Heading: "Your Benchmark Report is Ready"
    - Body text: "We've finished benchmarking {modelCount} vision models against your {imageCount} sample images."
    - Key result highlight: "Top recommendation: {recommendedModel}" (if available)
    - CTA button: "View Your Report" linking to `${siteUrl}/report/${shareToken}`
    - Footer: "You received this email because you purchased a ModelPick benchmark report."
    - Styling: inline styles compatible with email clients, dark background (#1a1a2e), light text, ember-orange accent for CTA button

    Props interface:
    ```typescript
    interface ReportReadyEmailProps {
      modelCount: number;
      imageCount: number;
      recommendedModel: string | null;
      reportUrl: string;
    }
    ```

    **2. Email sender (`src/lib/email/send-report-ready.ts`):**
    ```typescript
    export async function sendReportReadyEmail(params: {
      to: string;
      reportId: string;
      shareToken: string;
      modelCount: number;
      imageCount: number;
      recommendedModel: string | null;
    }): Promise<void>
    ```

    Implementation:
    - Check `isMockEmail()` from debug config. If mock mode:
      - console.log the email details (to, subject, key data) with `[email:mock]` prefix
      - Return without sending
    - Real mode:
      - Import Resend client: `new Resend(process.env.RESEND_API_KEY)`
      - Import the React Email template
      - Build report URL: `${process.env.NEXT_PUBLIC_SITE_URL}/report/${shareToken}`
      - Send via resend.emails.send():
        - from: 'ModelPick <noreply@your-domain.com>' (or use Resend's default sender for development: 'onboarding@resend.dev')
        - to: params.to
        - subject: 'Your ModelPick Benchmark Report is Ready'
        - react: ReportReadyEmail component with props
      - Wrap in try/catch -- email failure should NOT fail the benchmark. Log error and continue.
      - Log success: `[email] Report ready email sent to ${params.to}`

    **3. Wire email into engine.ts:**
    In the engine's Step 7 (complete report):
    - After updating report status to 'complete'
    - Look up user email from auth.users via admin client: `supabase.auth.admin.getUserById(report.user_id)`
    - Read the report's share_token (already generated by the webhook handler in Plan 02-01)
    - Call sendReportReadyEmail with user email, report details, and share_token for the report URL
    - Handle errors gracefully (log, don't fail -- email failure is non-fatal)
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - `npm run build` succeeds
    - Email template renders (can verify with `npx react-email export src/emails/report-ready.tsx` if available, otherwise just TypeScript check)
    - send-report-ready.ts exports sendReportReadyEmail
    - Mock mode logs email instead of sending
    - Engine.ts calls sendReportReadyEmail on completion
  </verify>
  <done>
    Report completion email sends via Resend with React Email template showing model count, recommended model, and CTA button linking to the report. Mock mode logs email details for development. Email failure is non-fatal -- benchmark results are preserved regardless of email delivery.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `npm run build` succeeds
3. `npm test` passes all existing tests
4. Engine can run end-to-end in mock mode: set DEBUG_MOCK_OPENROUTER=true, DEBUG_MOCK_EMAIL=true
5. Database receives benchmark_run records during execution
6. Report transitions through correct status states: paid -> running -> complete
7. Cost tracker prevents overspend (verify log output shows cost tracking)
8. Email template compiles without errors
9. Engine handles errors gracefully (individual run failures don't crash entire benchmark)
10. Engine respects concurrency limits (verify via log output timing)
</verification>

<success_criteria>
- runBenchmark() executes all model+image combinations with concurrency control
- Per-model concurrency capped at 3, global at 10 via p-limit
- Cost ceiling enforcement aborts cleanly at $7 soft / $15 hard limit
- Graceful shutdown at 750s elapsed time, skipping remaining runs
- Each run recorded to benchmark_runs table with all metrics
- Aggregate results calculated: accuracy, cost, latency, spread per model
- Recommended model determined from results
- Report completion email sent via Resend (or logged in mock mode)
- Full mock mode works without any external API keys
</success_criteria>

<output>
After completion, create `.planning/phases/02-pay-and-run/02-03-SUMMARY.md`
</output>
