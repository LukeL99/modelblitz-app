---
phase: 02-pay-and-run
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - src/lib/benchmark/engine.ts
  - src/lib/email/send-report-ready.ts
  - src/emails/report-ready.tsx
  - src/app/api/webhooks/stripe/route.ts
autonomous: true
user_setup:
  - service: resend
    why: "Transactional email delivery for report completion notification"
    env_vars:
      - name: RESEND_API_KEY
        source: "Resend Dashboard -> API Keys -> Create API Key"
    dashboard_config:
      - task: "Verify sending domain (or use onboarding@resend.dev for testing)"
        location: "Resend Dashboard -> Domains"
  - service: openrouter
    why: "Vision model API calls for benchmark execution"
    env_vars:
      - name: OPENROUTER_API_KEY
        source: "OpenRouter Dashboard -> Keys -> Create Key"
    dashboard_config:
      - task: "Add credits to OpenRouter account (minimum $10 recommended for testing)"
        location: "OpenRouter Dashboard -> Credits"

must_haves:
  truths:
    - "Benchmark engine iterates over all selected models and all images with p-limit concurrency control"
    - "Engine tracks cost per call and aborts remaining work when soft ceiling ($6.50) is reached"
    - "Engine records each run result (JSON output, timing, tokens, cost, pass/fail, field diffs) to benchmark_runs table"
    - "Engine updates report status to 'complete' with total API cost and recommended model on success"
    - "Engine updates report status to 'failed' on unrecoverable error"
    - "Report completion email is sent to user with report link after successful benchmark"
    - "Webhook handler calls real runBenchmark instead of placeholder"
  artifacts:
    - path: "src/lib/benchmark/engine.ts"
      provides: "Benchmark orchestration loop with concurrency, cost ceiling, DB writes, and report finalization"
      exports: ["runBenchmark"]
    - path: "src/lib/email/send-report-ready.ts"
      provides: "Send report completion email via Resend"
      exports: ["sendReportReadyEmail"]
    - path: "src/emails/report-ready.tsx"
      provides: "React Email template for report completion notification"
  key_links:
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/benchmark/runner.ts"
      via: "runModelOnce for each model+image combination"
      pattern: "runModelOnce"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/benchmark/json-compare.ts"
      via: "jsonMatch and fieldDiff for accuracy scoring"
      pattern: "jsonMatch.*fieldDiff"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/benchmark/cost-tracker.ts"
      via: "CostTracker for budget enforcement"
      pattern: "CostTracker"
    - from: "src/lib/benchmark/engine.ts"
      to: "src/lib/email/send-report-ready.ts"
      via: "sendReportReadyEmail after successful completion"
      pattern: "sendReportReadyEmail"
    - from: "src/app/api/webhooks/stripe/route.ts"
      to: "src/lib/benchmark/engine.ts"
      via: "after() calling runBenchmark"
      pattern: "import.*runBenchmark.*from.*engine"
---

<objective>
Build the benchmark engine orchestrator that runs all selected models against all images with concurrency control and cost ceiling enforcement, then sends a completion email. Wire the real engine into the webhook handler replacing the placeholder.

Purpose: This is the core execution engine that delivers the product value -- actually running benchmarks across vision models, comparing outputs, and producing results. The email notifies users when results are ready.
Output: Engine module, email sender, email template, updated webhook handler.
</objective>

<execution_context>
@/Users/lukelibraro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lukelibraro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Research patterns for engine, email, orchestration
@.planning/phases/02-pay-and-run/02-RESEARCH.md

# Plan 02-01 SUMMARY (Stripe/webhook infra, DB schema, admin client)
@.planning/phases/02-pay-and-run/02-01-SUMMARY.md

# Plan 02-02 SUMMARY (json-compare, backoff, cost-tracker, runner)
@.planning/phases/02-pay-and-run/02-02-SUMMARY.md

# Existing files this plan uses
@src/lib/benchmark/runner.ts
@src/lib/benchmark/json-compare.ts
@src/lib/benchmark/cost-tracker.ts
@src/lib/config/models.ts
@src/lib/config/constants.ts
@src/types/database.ts
@src/lib/supabase/admin.ts
@src/app/api/webhooks/stripe/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Benchmark engine orchestration with concurrency, cost ceiling, and DB persistence</name>
  <files>src/lib/benchmark/engine.ts</files>
  <action>
    Create `src/lib/benchmark/engine.ts` following research Pattern 7. Export `runBenchmark(reportId: string): Promise<void>`.

    **Imports:**
    - `pLimit` from `p-limit` (dynamic import: `const pLimit = (await import("p-limit")).default` since p-limit is ESM-only and may need dynamic import in Next.js server context. Test static import first: `import pLimit from "p-limit"`. If build fails, switch to dynamic import inside the function.)
    - `createAdminClient` from `@/lib/supabase/admin`
    - `CostTracker`, `estimateCost` from `./cost-tracker`
    - `runModelOnce` from `./runner`
    - `jsonMatch`, `fieldDiff` from `./json-compare`
    - `CURATED_MODELS` from `@/lib/config/models`
    - `COST_SOFT_CEILING`, `PER_MODEL_CONCURRENCY`, `GLOBAL_CONCURRENCY`, `MAX_BENCHMARK_DURATION_S`, `DEFAULT_INPUT_TOKENS`, `DEFAULT_OUTPUT_TOKENS` from `@/lib/config/constants`

    **Helper: `getModelById(modelId: string)`** -- find model in `CURATED_MODELS` by `id`. Return `ModelInfo | undefined`.

    **Main function `runBenchmark(reportId: string)`:**

    1. Create admin Supabase client and CostTracker with `COST_SOFT_CEILING` ceiling.
    2. Record `startTime = Date.now()`.
    3. Update report status to `"running"` with `started_at: new Date().toISOString()`.
    4. Load full report record from DB.
    5. Extract from `report.config_snapshot`:
       - `selectedModels: string[]` from `config_snapshot.selected_models` (array of model IDs)
       - `images` array from `config_snapshot.upload_data?.images`
       - `expectedJsons`: parse each image's `expectedJson` string into an object
       - `imageUrls`: extract each image's `publicUrl`
       - `extractionPrompt`: from `config_snapshot.schema_data?.prompt`
    6. Create global concurrency limiter: `const globalLimit = pLimit(GLOBAL_CONCURRENCY)`.
    7. Create per-model limiters: `Map<string, ReturnType<typeof pLimit>>` -- for each modelId in selectedModels, create `pLimit(PER_MODEL_CONCURRENCY)`.
    8. Build task list: for each modelId, for each imageIndex:
       - Wrap in `globalLimit(() => modelLimit(async () => { ... }))` for double concurrency control
       - Before each call: check `costTracker.isExhausted()` -- if true, skip
       - Before each call: check elapsed time `(Date.now() - startTime) / 1000 > MAX_BENCHMARK_DURATION_S` -- if true, skip (graceful timeout)
       - Before each call: estimate cost with `estimateCost(model, DEFAULT_INPUT_TOKENS, DEFAULT_OUTPUT_TOKENS)`. If `!costTracker.canAfford(estimatedCost)`, skip.
       - Call `runModelOnce(model, imageUrls[imgIdx], extractionPrompt)`
       - After call: `costTracker.addCost(result.cost)` (actual cost from model pricing calculation)
       - Compare: `passed = result.success ? jsonMatch(expectedJsons[imgIdx], result.parsedOutput) : false`
       - Field diffs: `diffs = result.success && result.parsedOutput && typeof result.parsedOutput === "object" ? fieldDiff(expectedJsons[imgIdx], result.parsedOutput as Record<string, unknown>) : []`
       - Insert `benchmark_runs` record via admin client with: report_id, model_id, image_index, output_json, response_time_ms (Math.round), prompt_tokens, completion_tokens, total_tokens, cost, passed, field_diffs, error
    9. Execute all tasks with `Promise.allSettled(tasks)` (allSettled, not all -- individual failures do not abort other tasks).
    10. **Determine recommended model:** Query benchmark_runs for this report, group by model_id, calculate accuracy per model (count passed / count total), find model with highest accuracy. On ties, prefer lower cost.
    11. **Finalize report:** Update report with `status: "complete"`, `total_api_cost: costTracker.getSpent()`, `model_count: selectedModels.length`, `recommended_model: topModel?.model_id || null`, `completed_at: new Date().toISOString()`.
    12. **Send completion email:** Import and call `sendReportReadyEmail` (from plan's Task 2). Get user email by querying `auth.users` via admin client or storing email during report creation. Pass report share_token, top model name, and accuracy percentage. Wrap in try/catch -- email failure should NOT fail the report.

    **Error handling:** Wrap the entire function body in try/catch. On error:
    - `console.error("[benchmark] Failed:", err)`
    - Update report: `status: "failed"`, `total_api_cost: costTracker.getSpent()`, `completed_at: new Date().toISOString()`

    **Logging:** Add `console.log` statements at key points: starting benchmark, each model completion, cost ceiling reached, timeout reached, benchmark complete.
  </action>
  <verify>
    1. `npm run build` passes
    2. `grep "export async function runBenchmark" src/lib/benchmark/engine.ts` confirms export
    3. `grep "pLimit" src/lib/benchmark/engine.ts` confirms concurrency control
    4. `grep "costTracker.isExhausted" src/lib/benchmark/engine.ts` confirms cost ceiling check
    5. `grep "Promise.allSettled" src/lib/benchmark/engine.ts` confirms task execution
    6. `grep "jsonMatch" src/lib/benchmark/engine.ts` confirms accuracy comparison
    7. `grep "fieldDiff" src/lib/benchmark/engine.ts` confirms field-level diff capture
    8. `grep "benchmark_runs" src/lib/benchmark/engine.ts` confirms DB writes
    9. `grep "sendReportReadyEmail" src/lib/benchmark/engine.ts` confirms email integration
  </verify>
  <done>
    Engine orchestrates all model x image combinations with global (10) and per-model (3) concurrency limits. Checks cost ceiling before each call. Gracefully times out before 800s limit. Records each run result to benchmark_runs. Determines recommended model by accuracy. Finalizes report with total cost and status. Sends completion email. Handles errors by marking report as failed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Report completion email and webhook wiring</name>
  <files>
    src/lib/email/send-report-ready.ts
    src/emails/report-ready.tsx
    src/app/api/webhooks/stripe/route.ts
  </files>
  <action>
    **Create `src/emails/report-ready.tsx`:**
    React Email template using `@react-email/components`. Import: `Html`, `Head`, `Body`, `Container`, `Section`, `Text`, `Button`, `Hr`, `Preview` from `@react-email/components`.

    Props interface: `{ reportUrl: string; topModel: string; accuracy: number }`.

    Template renders:
    - Preview text: "Your ModelPick benchmark is complete"
    - Simple, clean layout with white/light background (email clients render dark mode inconsistently, so use light theme for emails)
    - Heading: "Your Benchmark Report is Ready"
    - Text: "We've finished benchmarking your vision models."
    - Top result callout: "Top Model: {topModel}" with "Accuracy: {accuracy}%"
    - CTA button: "View Full Report" linking to reportUrl
    - Footer: "ModelPick - Find the best vision model for your extraction pipeline"

    Keep it simple -- minimal styling, no complex layout. Email rendering is unreliable.

    Export as default: `export default function ReportReadyEmail(props: ReportReadyEmailProps)`

    **Create `src/lib/email/send-report-ready.ts`:**
    Import `Resend` from `resend`.
    Import `ReportReadyEmail` from `@/emails/report-ready`.

    Create Resend client: `const resend = new Resend(process.env.RESEND_API_KEY)`.

    Export `sendReportReadyEmail(userEmail: string, reportId: string, shareToken: string, topModel: string, accuracy: number): Promise<void>`:
    1. Construct `reportUrl = ${process.env.NEXT_PUBLIC_SITE_URL}/report/${shareToken}`
    2. Call `resend.emails.send()` with:
       - `from: "ModelPick <noreply@modelpick.ai>"` (in development, Resend's onboarding domain is used automatically)
       - `to: userEmail`
       - `subject: \`Your benchmark report is ready - ${topModel} won at ${accuracy}%\``
       - `react: ReportReadyEmail({ reportUrl, topModel, accuracy })`
    3. Log success or error. Do NOT throw on email failure -- the report is already complete.

    **Update `src/app/api/webhooks/stripe/route.ts`:**
    Replace the placeholder `runBenchmark` function with the real import:
    - Remove the placeholder function that just logs a message
    - Add `import { runBenchmark } from "@/lib/benchmark/engine"` at the top
    - The `after(() => runBenchmark(report.id))` call should already be in place from Plan 02-01
    - Verify the import path is correct and the function signature matches

    This is the critical wiring that connects payment -> benchmark execution.
  </action>
  <verify>
    1. `npm run build` passes
    2. `ls src/emails/report-ready.tsx src/lib/email/send-report-ready.ts` confirms files exist
    3. `grep "import { runBenchmark }" src/app/api/webhooks/stripe/route.ts` confirms real import (not placeholder)
    4. `grep "resend.emails.send" src/lib/email/send-report-ready.ts` confirms Resend integration
    5. `grep "ReportReadyEmail" src/emails/report-ready.tsx` confirms template export
    6. No placeholder `console.log("[benchmark] Would run benchmark"` remains in webhook handler
  </verify>
  <done>
    React Email template renders report completion notification with top model results and CTA button. Resend sends email to user's address. Webhook handler imports real runBenchmark from engine module. Full payment -> webhook -> benchmark -> email pipeline is wired end-to-end.
  </done>
</task>

</tasks>

<verification>
1. Build passes: `npm run build` exits 0
2. Engine imports and composes all utility modules from Plan 02-02
3. Engine uses p-limit for both global and per-model concurrency
4. Engine checks cost ceiling before each API call
5. Engine records results to benchmark_runs table
6. Engine determines recommended model and finalizes report
7. Email is sent on successful completion (non-blocking on failure)
8. Webhook handler uses real runBenchmark import (no placeholder)
9. Full chain: Stripe webhook -> report creation -> after() -> runBenchmark -> results -> email
</verification>

<success_criteria>
- runBenchmark orchestrates model x image matrix with concurrent execution
- Cost ceiling prevents runaway spending (soft ceiling at $6.50)
- Graceful timeout prevents Vercel function kill (750s threshold)
- Each run result stored in benchmark_runs with JSON output, timing, tokens, cost, pass/fail, field diffs
- Report finalized with total cost, recommended model, and 'complete' status
- Completion email sent with report link and top model results
- Webhook imports and calls the real engine (not a placeholder)
- Build passes with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-pay-and-run/02-03-SUMMARY.md`
</output>
